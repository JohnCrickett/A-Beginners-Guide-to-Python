{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Testing\n",
    "\n",
    "Testing is an easy thing to understand but there is also an art to it as well; writing good tests often requires you to try to figure out *what input(s) is most likely to break my program*. \n",
    "\n",
    "In addition to this, tests can serve different purposes as well:\n",
    "\n",
    "* Testing for correctness\n",
    "* Testing for bugs\n",
    "* Testing for \"Let's check I didn't fuck something up\"  (a.k.a 'regression testing')\n",
    "* ...etc...\n",
    "\n",
    "All of the above tests have their uses, but as a general rule of thumb a good test suite will include a range of inputs and multiple tests for each. \n",
    "\n",
    "I would add a small caveat that if there is documentation for a function that says something like \"does not work for strings\" then although its possible to write test code for strings what would be the point? The documentation makes it clear that these tests will fail. Instead of writing test code for situations the code was **not designed to solve** focus on 'realistic' test cases.\n",
    "\n",
    "Alright, lets write a super simple function that divides A by B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide(a, b):\n",
    "    \"\"\"\"a, b are ints or floats. Returns a/b\"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOkay so, this is where we need to put our ‘thinking hat’ on for a moment. The documentation for this function specifically states A and B are supposed to be numbers, so instead of wasting time breaking the code with obviously bad inputs lets try to break with valid inputs. In other words: \n",
    "\n",
    "> what are the possible integers/floats we can pass in where this function may break?\n",
    "\n",
    "When dealing with numbers there are, I think, three basic tests that are almost always worth running:\n",
    "\n",
    "1. Negative Numbers\n",
    "1. Zero\n",
    "1. Positive Numbers\n",
    "\n",
    "And in addition to those tests we should also run tests for:\n",
    "\n",
    "1. Small inputs (10/5)\n",
    "1. Very large inputs ( 999342493249234234234234 / 234234244353452424 )\n",
    "\n",
    "You may remember for example that as we changed our is_prime function to be faster we had some issues with small inputs. \n",
    "\n",
    "Anyway, the point is these five basic cases will cover a lot of situations you may have with numbers. Obviously you should run several tests for each of these basic test cases. **And in addition to the basic tests you should run more function specific tests too**; for example, if I have a function that returns the factors of n then it would be wise to run a bunch of tests with prime numbers to check what happens there. You should also test [*highly composite numbers*](https://en.wikipedia.org/wiki/Highly_composite_number) too (e.g 720, 1260). In regard to our division function a good additional test would be when the numerator is larger than the denominator and vice versa (e.g. try both 10/2 and 2/10). Zero is also a special case for division, but we have already listed it in the basic tests.  \n",
    "\n",
    "Okay, so lets write our first tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Function here...\n",
    "print (divide(10, 2) == 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so our first test is simply asking if divide (10/2) returns 5.0. If \"True\" gets printed to the console we know the test passes. \n",
    "\n",
    "Okay, lets write a few more example cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[divide(10.0,2.0) == 5.0, divide(10,2) == 5.0, divide(0,1) == 0.0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know that X/0 is a ZeroDivisionError, the question is when we do our tests what to we want the result to be? Do we want python to raise the error? or would we prefer Python to do something else such as return the string \"division by zero not computable\".\n",
    "\n",
    "Remember that errors are not bad, if we what Python to throw an error when it gets zero as input that’s totally fine, and in this case I think I’m happy with the error. This means I have to write a test case that expects an error to be raised. We can do that like so…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    divide(1, 0)\n",
    "    print(False)\n",
    "except ZeroDivisionError:\n",
    "    print(True) # Test pass, dividing by zero yields an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, next up we need to test for large numbers. When it came to small numbers we can easily work out the correct answer by hand, but for large sums that’s not so easy. \n",
    "\n",
    "Your first instinct here might be to say \"use a calculator\" and while that’s true, that solution only works in this very specific case. What we actually want is a more general solution that can solve all sorts of problems. \n",
    "\n",
    "It turns out that sometimes building code that can generate test cases is a lot easier that building the solver. In this particular example we can do just that...\n",
    "\n",
    "Let's take a step back and ask ourselves what division actually is. The answer is basically the opposite of multiplication. And so, we can actually write test cases for our function by \"reverse engineering\" the problem. We know from math that the following is always true: \n",
    "\n",
    "    (y * y) / y = y\n",
    "    (x * y) / y = x\n",
    "\n",
    "And so, so long as we have a function that multiplies correctly, we can be confident that our function is getting the right answer to complex division problems *even though* we do not know what the right answer is ourselves. In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 30202020202020202022424354265674567456\n",
    "y = 95334534534543543543545435543543545345\n",
    "\n",
    "divide(y * y, y) == float(y)\n",
    "divide(x * y, y) == float(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was an easy example, but you'd be surprised how often you can generate tests for functions, and that’s why this technique comes in useful. \n",
    "\n",
    "## Homework assignment\n",
    "\n",
    "Get_primes is a function that takes a list, L, of numbers and return a list of all the primes found within L. Your task is to finish the ‘test_it’ function below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_primes(l):\n",
    "    # This function is broken (but please dont fix it!).\n",
    "    return l\n",
    "\n",
    "def get_primes_correct(l):\n",
    "    # correct implementation of get_primes\n",
    "    primes = []\n",
    "    for num in l:\n",
    "        if num == 2:\n",
    "            primes.append(num)\n",
    "        elif num % 2 == 0:\n",
    "            continue\n",
    "        else:\n",
    "            for n in range(3, math.ceil(math.sqrt(num))+1, 2):\n",
    "                if num % n == 0:\n",
    "                    continue\n",
    "            primes.append(num)\n",
    "    return primes\n",
    "\n",
    "### YOUR CODE HERE #### \n",
    "def test_it():\n",
    "    \"\"\"returns a very large list of numbers (1000-to-10,000) and the expected solution when we call Get_primes on that list\"\"\"\n",
    "    # Your code here (note the return statement has already been done for you!)\n",
    "    # l = the big list, solution = a list containing all the primes in l. \n",
    "    return (l, solution)\n",
    "    \n",
    "l, solution = test_it()\n",
    "solution.sort()\n",
    "print(sorted(get_primes(l)) == solution) # This test should fail.\n",
    "print(sorted(get_primes_correct(l)) == solution) # This test should pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so that’s testing of a very simple function called divide. Next up, lets talk about how to generate hundreds of tests...\n",
    "\n",
    "## Testing in the Thousands\n",
    "\n",
    "Generally speaking the more tests you do the better. A mere ten tests does not inspire confidence that our code works. What we really want is to run hundreds of tests, but we certainly don't want to write those test cases out by hand!\n",
    "\n",
    "We can use for-loops to call a function hundreds of times with different inputs, but there's a catch: what can we actually test for?\n",
    "\n",
    "If we know what the answer should be in each case then we can test for correctness. But if we do not know what the answer should be then we can't test for correctness, but it is possible to check if the code will throw errors or something. \n",
    "\n",
    "I'm going to show you both approaches. But before I do that, lets create a function that doesn't work (some of the time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bad_divide(a, b):\n",
    "    # our testing function that tests our tests! \n",
    "    # this function should work for all values EXCEPT when a > 500,000\n",
    "    if a > 500000:\n",
    "        return a\n",
    "    else:\n",
    "        return a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and now let's write code to test for correctness..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_ints():\n",
    "    a = random.choice(range(1, 750))\n",
    "    b = random.choice(range(1, a))  # b is always smaller than a\n",
    "    return a, b \n",
    "\n",
    "def run_test(n):\n",
    "    for test in range(n):\n",
    "        a, b = random_ints()\n",
    "        if bad_divide(a*b, b) != float(a):\n",
    "            return \"TEST NO {}: A*B was: {} B was: {} Result was: {} Expected result was: {}\".format(test, a*b, b, \n",
    "                                                                                       bad_divide(a*b, b), float(a))\n",
    "    return \"ALL {} TESTS PASSED\".format(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright so we have our test function and using the math trick discussed above we can test for correctness. lets give it a spin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ALL 5 TESTS PASSED'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_test(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we five random tests and we pass them all, Great! Well actually not so great, we happen to know that bad divide is actually an incorrect function and so its bad news that our test functions failed to pick up on that. Let’s give our test function a bit more time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST NO 58: A*B was: 550548 B was: 738 Result was: 550548 Expected result was: 746.0'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_test(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code ran for a while and it wasn't until the 58th test it finally broke down. And since we thought to save the data we now have a starting point to troubleshoot the problem. Knowing under what circumstances our code fails is the first step towards fixing it.\n",
    "\n",
    "However, as I mentioned before we can only test for correctness by the bucket-load if we have some function capable of know what the correct output should be. \n",
    "\n",
    "If we can't do that, then the only thing we can do with large random tests is test the code isn't throwing up any surprises.\n",
    "\n",
    "For example, we could use a line like:\n",
    "    \n",
    "    isinstance(function, {type})\n",
    "\n",
    "Code such as this couldn’t prove a function correct but it will potentially catch a bug or two. In this particular case, we have a function that is supposed to return lists, if there is some input out there that makes the function return strings instead we would want to know about it.\n",
    "\n",
    "Another thing you can do is make educated guesses as to what the correct answer should look like. For example, in the case of the divide function above we could run hundreds of tests where we check:\n",
    "    \n",
    "    divide(a, b) >= a.  \n",
    "    \n",
    "That makes sense right? if our function is dividing A by B then, even if we don't know what the answer should be we do know what the result should always be less than the number we started with. \n",
    "\n",
    "This kinds of tests can sometimes be useful, notice also that if the operator (in this case ‘>’) doesn’t work on some inputs you get the added bonus of **sometimes** getting an error when the wrong data-type is given as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bad_list(n):\n",
    "    \"\"\"Returns a list 0 to n. \"\"\"\n",
    "    if n < 100:\n",
    "        return [7] + list(range(n))\n",
    "    elif n > 150:\n",
    "        return list(range(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, just as before, we can clearly see where and how this function will misbehave, but lets run a few tests and see if our tests can actually pin-point the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def test_list_func(n):\n",
    "    for test in range(n):\n",
    "        randomint = random.choice(range(0,200))\n",
    "        if not isinstance(bad_list(randomint), list):\n",
    "            \n",
    "            print(\"Test isinstance #{}: Failed, input: {}, output: {}\".format(test, randomint, bad_list(randomint)))\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            # Smart assumption test, max of list shouldn't be greater than n\n",
    "            max_test = max(bad_list(randomint)) > randomint\n",
    "            if max_test:\n",
    "                print(\"Test Max #{}: Failed. input: {}\".format(test, randomint))\n",
    "                break\n",
    "    return \"TESTING COMPLETE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this code is completing two tests at the same time. Firstly we are checking if the result is always a list. The second test is an intuitive one; if our function is supposed to return 0-to-N then clearly something has gone wrong is the max of the list is greater than n. \n",
    "\n",
    "To once again reiterate, this tests are not designed to prove correctness, but they are capable of finding a few bugs we would probably like to address. okay, lets run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test isinstance #4: Failed, input: 145, output: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TESTING COMPLETE'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list_func(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so test 4 fails, it turns out if we input 145, the function does not return a list, rather, it returns None instead. If you re-read the function this makes a lot of sense, we are asking if N < 100 or N > 150. But if N falls in the range 101-149 then both the if and elif statements are False and there isn't an else clause, so Python literally does nothing and our test function flags up the error. \n",
    "\n",
    "Lets run the test again and see if we can catch the other error..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Max #20: Failed. input: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TESTING COMPLETE'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list_func(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So on the 20th test we get a \"max test fail\" on the input 5. And since we made the function terrible on purpose we actually know what happened. The number 7 gets added to the output and since 7 > 5 the max test flags up an error.\n",
    "\n",
    "## Doctesting\n",
    "\n",
    "The last thing to cover today is something called doctesting, this is a really quick way to making quick tests for your program.\n",
    "    \n",
    "    The Syntax:\n",
    "    \"\"\" \n",
    "    >>> {function name} ( {function argument, if any} )\n",
    "    {expected result}\n",
    "    \"\"\"\n",
    "\n",
    "And then once you have done that, you'll need to copy & paste the code below to run the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_doctests():\n",
    "    import doctest\n",
    "    doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default if all your tests pass nothing will be printed, but should a doctest fail Python will give you all the juicy detail. Lets try it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    \"\"\"Returns a + b\n",
    "    \n",
    "    >>> add(10, 10)\n",
    "    20\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "run_doctests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran doctests, but since the test past nothing happened. Alright, lets show you want happens on failure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "File \"__main__\", line 3, in __main__.run_all_the_tests\n",
      "Failed example:\n",
      "    bad_list(4)\n",
      "Expected:\n",
      "    [0, 1, 2, 3]\n",
      "Got:\n",
      "    [7, 0, 1, 2, 3]\n",
      "**********************************************************************\n",
      "File \"__main__\", line 12, in __main__.run_all_the_tests\n",
      "Failed example:\n",
      "    20 + 2\n",
      "Expected:\n",
      "    23  \n",
      "Got:\n",
      "    22\n",
      "**********************************************************************\n",
      "1 items had failures:\n",
      "   2 of   4 in __main__.run_all_the_tests\n",
      "***Test Failed*** 2 failures.\n"
     ]
    }
   ],
   "source": [
    "def run_all_the_tests():   \n",
    "    \"\"\"\n",
    "    >>> bad_list(4)\n",
    "    [0, 1, 2, 3]\n",
    "    \n",
    "    >>> 1 + 1\n",
    "    2\n",
    "    \n",
    "    >>> print(True)\n",
    "    True\n",
    "    \n",
    "    >>> 20 + 2\n",
    "    23  \n",
    "    \"\"\"   \n",
    "\n",
    "    print(\"testing complete\")\n",
    "    \n",
    "run_doctests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Python ran four tests and two of them failed. It turns out 20 + 2 does not equal 23 and bad_list (surprise surprise) it up to no good. \n",
    "\n",
    "Overall, I'd recommend beginners use doctesting. Its fairly easy to use and it allows you to quickly type out basic tests for your functions.\n",
    "\n",
    "As a matter of fact, doctests are a great thing to write **BEFORE** you write the rest of your code. By spending a minute typing out a few basic test cases can help you write your code better. Why is that? Well, if you are thinking about good tests (i.e. stuff likely to break your code) at the outset, chances are you will code with that problem in mind.\n",
    "\n",
    "Alright guys, that’s everything I think I need to say about testing. If you only learn one thing from today’s lecture please let it be thing: The secret to writing good test cases is thinking hard about all the wonderful way the code could break. \n",
    "\n",
    "p.s.DONT FORGET YOUR HOMEWORK!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
