{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Testing\n",
    "\n",
    "Testing is an easy thing to understand but there is also an art to it as well; writing good tests often requires you to try to figure out *what input(s) is most likely to break your program*. \n",
    "\n",
    "In addition to this, tests can serve different purposes as well:\n",
    "\n",
    "* Testing for correctness\n",
    "* Testing for bugs\n",
    "* Testing for \"Let's check I didn't fuck something up\"  (a.k.a 'regression testing')\n",
    "* ...etc...\n",
    "\n",
    "All of the above tests have their uses, but as a general rule of thumb a good test suite will include a range of inputs and multiple tests for each. \n",
    "\n",
    "I would add a small caveat that if there is documentation for a function that says something like \"does not work for strings\" then although it is possible to write test code for strings what would be the point? The documentation makes it clear that these tests will fail. Instead of writing test code for situations the code was **not designed to solve** focus on 'realistic' test cases.\n",
    "\n",
    "Alright, lets write a super simple function that divides A by B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide(a, b):\n",
    "    \"\"\"\"a, b are ints or floats. Returns a/b\"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so, this is where we need to put our ‘thinking hat’ on for a moment. The documentation for this function specifically states A and B are supposed to be numbers, so instead of wasting time breaking the code with obviously bad inputs lets try to break with valid inputs. In other words: \n",
    "\n",
    "> what are the possible integers/floats we can pass in where this function may break?\n",
    "\n",
    "When dealing with numbers there are, I think, three basic tests that are almost always worth running:\n",
    "\n",
    "1. Negative Numbers\n",
    "1. Zero\n",
    "1. Positive Numbers\n",
    "\n",
    "And in addition to those tests we should also run tests for:\n",
    "\n",
    "1. Small inputs (10/5)\n",
    "1. Very large inputs ( 999342493249234234234234 / 234234244353452424 )\n",
    "\n",
    "You may remember for example in lecture 21 as we tried to optimise our is_prime function we introduced some defects when working with small numbers.\n",
    "\n",
    "Anyway, the point is these five basic cases will cover a lot of situations you may have with numbers. Obviously you should run several tests for each of these basic test cases. **And in addition to the basic tests you should run more function specific tests too**; for example, if I have a function that returns the factors of n then it would be wise to run a bunch of tests with prime numbers to check what happens there. You should also test [*highly composite numbers*](https://en.wikipedia.org/wiki/Highly_composite_number) too (e.g 720, 1260). In regard to our division function a good additional test would be when the numerator is larger than the denominator and vice versa (e.g. try both 10/2 and 2/10). Zero is also a special case for division, but we have already listed it in the basic tests.  \n",
    "\n",
    "Okay, so lets write our first tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True, True, True]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function here...\n",
    "print (divide(10, 2) == 5.0)\n",
    "[divide(10.0, 2.0) == 5.0, divide(10,2) == 5.0, divide(0, 1) == 0.0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know that X/0 is a ZeroDivisionError, the question is when we do our tests what do we want the result to be? Do we want Python to raise the error? or would we prefer Python to do something else such as return a number or perhaps a string.\n",
    "\n",
    "Remember that errors are not bad, if Python to throws an error when it gets zero as input that’s totally fine, and in this case I think I’m happy with the error. This means I have to write a test case that **expect** an error to be raised. We can do that like so…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    divide(1, 0)\n",
    "    print(False) # note that if the above line of code yields a zeroDiv error, this line of code is not executed. \n",
    "except ZeroDivisionError:\n",
    "    print(True) # Test pass, dividing by zero yields an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, next up we need to test for large numbers. When it came to small numbers we can easily work out the correct answer by hand, but for large sums that’s not so easy. \n",
    " \n",
    "Your first instinct here might be to say \"use a calculator\" and while that’s true, that solution only works in this very specific case. What we actually want is a more general solution that can solve all sorts of problems. \n",
    " \n",
    "It turns out that sometimes building code that can generate test cases is a lot easier that building the solver. In this particular example we can do just that...\n",
    " \n",
    "Let's take a step back and ask ourselves what division actually is. The answer is basically the opposite of multiplication. And so, we can actually write test cases for our function by \"reverse engineering\" the problem. We know from math that the following is always true: \n",
    " \n",
    "    (y * y) / y = y\n",
    "    (x * y) / y = x\n",
    " \n",
    "And so, so long as we have a function that multiplies correctly, we can be confident that our function is getting the right answer to complex division problems *even though* we do not know what the right answer is ourselves. In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 30202020202020202022424354265674567456\n",
    "y = 95334534534543543543545435543543545345\n",
    "\n",
    "divide(y * y, y) == float(y)\n",
    "divide(x * y, y) == float(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Testing n/5 function...\n",
    "\n",
    "To make sure you understand the concept I’m trying to teach let’s try another example. Suppose we have a function that checks if a number is divisible by 5.  How can we write a tests that can prove our function works **WITHOUT** using division or modular arithmetic in our test cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...TESTING COMPLETE...\n"
     ]
    }
   ],
   "source": [
    "def div5(x):\n",
    "    return x % 5 == 0\n",
    "\n",
    "# Test 1; positive cases\n",
    "for i in range(-1000, 1000, 5):\n",
    "    # remember the third argument to the range function is the step size.\n",
    "    if div5(i) == False:\n",
    "        print(\"TEST FAILED\")\n",
    "\n",
    "print(\"...TESTING COMPLETE...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the above test checks whether our function correctly identifies numbers divisible by 5. However this is only half the job done. Let's build something to test for numbers that are not divisible by 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593\n",
      "805685903559833387139607\n",
      "468131808\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def build_number(size):\n",
    "    \"\"\"\n",
    "    This function returns a number that has n (or n+1) digits that is NOT divisible by 5.\n",
    "    \"\"\"\n",
    "    \n",
    "    legal_digits = \"0123456789\"\n",
    "    digits_2 = \"12346789\" # <= missing 0 and 5\n",
    "    \n",
    "    number = \"\"\n",
    "    for _ in range(size):\n",
    "        number += random.choice(legal_digits) # <= randomly choose a digit\n",
    "        \n",
    "        \n",
    "    if number[-1] in [\"5\", \"0\"]:\n",
    "        # if the last digit is 0 or 5, then the number is divisible by 5. \n",
    "        # Thus we randomly add another digit. \n",
    "        number += random.choice(digits_2)\n",
    "    \n",
    "    if number[0] == 0:\n",
    "        # If we have a leading zero, we get rid of it. \n",
    "        number = random.choice(digits_2) + number\n",
    "        \n",
    "    return int(number)\n",
    "\n",
    "print(build_number(3))\n",
    "print(build_number(23))\n",
    "print(build_number(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the build number function above creates random numbers but crucially for our purposes the last digit of these numbers CANNOT by either a 5 or 0. Thus, we have build a function that randomly generates numbers guaranteed not to be divisible by 5. And more importantly, nowhere does this code use division or modulo arithmetic itself.\n",
    "\n",
    "The final part of the puzzle then is to use this function to test div5..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...TESTING COMPLETE...\n"
     ]
    }
   ],
   "source": [
    "number_of_tests = 1000\n",
    "\n",
    "for test_case in range(number_of_tests):\n",
    "    number = build_number(random.choice(range(2,30)))\n",
    "    if div5(number):\n",
    "        print(\"Test Failed\")\n",
    "        \n",
    "print(\"...TESTING COMPLETE...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework assignment\n",
    "\n",
    "'get_primes' is a function that takes a list, L, of numbers and returns a list of all the primes found within L. Your task is to finish the ‘test_it’ function below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_primes(l):\n",
    "    # This function is broken (but please dont fix it!).\n",
    "    return l\n",
    "\n",
    "def get_primes_correct(l):\n",
    "    # correct implementation of get_primes\n",
    "    primes = []\n",
    "    for num in l:\n",
    "        if num == 2:\n",
    "            primes.append(num)\n",
    "        elif num % 2 == 0:\n",
    "            continue\n",
    "        else:\n",
    "            for n in range(3, math.ceil(math.sqrt(num))+1, 2):\n",
    "                if num % n == 0:\n",
    "                    continue\n",
    "            primes.append(num)\n",
    "    return primes\n",
    "\n",
    "### YOUR CODE HERE #### \n",
    "def test_it():\n",
    "    \"\"\"returns a very large list of numbers (1000-to-10,000) and the expected solution when we call Get_primes on that list\"\"\"\n",
    "    # Your code here (note the return statement has already been done for you!)\n",
    "    # l = the big list, solution = a list containing all the primes in l. \n",
    "    return (l, solution)\n",
    "    \n",
    "l, solution = test_it()\n",
    "solution.sort()\n",
    "print(sorted(get_primes(l)) == solution)         # This test should fail.\n",
    "print(sorted(get_primes_correct(l)) == solution) # This test should pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing in the Thousands\n",
    "\n",
    "Generally speaking the more tests you do the better. A mere ten tests does not inspire confidence that our code works. What we really want is to run hundreds of tests, but we certainly don't want to write those test cases out by hand!\n",
    "\n",
    "We can use for-loops to call a function hundreds of times with different inputs, but there's a catch: what can we actually test for?\n",
    "\n",
    "If we know what the answer should be in each case then we can test for correctness. But if we do not know what the answer should be then we can't test for correctness, but it is possible to check if the code will throw errors or something. \n",
    "\n",
    "I'm going to show you both approaches. But before I do that, lets create a function that doesn't work (some of the time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bad_divide(a, b):\n",
    "    # our testing function that tests our tests! \n",
    "    # this function should work for all values EXCEPT when a > 500,000\n",
    "    if a > 500000:\n",
    "        return a\n",
    "    else:\n",
    "        return a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and now let's write code to test for correctness..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_ints():\n",
    "    a = random.choice(range(10, 750))\n",
    "    b = random.choice(range(1, a))   # b is ALWAYS smaller than a\n",
    "    return a, b \n",
    "\n",
    "def run_test(n):\n",
    "    for test in range(n):\n",
    "        a, b = random_ints()\n",
    "        if bad_divide(a*b, b) != float(a):\n",
    "            return \"TEST NO {}: A*B was: {} B was: {} Result was: {} Expected result was: {}\".format(test, a*b, b, \n",
    "                                                                                       bad_divide(a*b, b), float(a))\n",
    "    return \"ALL {} TESTS PASSED\".format(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright so we have our test function and using the math trick discussed above we can test for correctness. lets give it a spin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ALL 5 TESTS PASSED'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_test(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we five random tests and we pass them all, Great! Well actually not so great, we happen to know that bad divide is actually an incorrect function and so its bad news that our test functions failed to pick up on that. Let’s give our test function a bit more time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ALL 100 TESTS PASSED'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_test(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ALL 300 TESTS PASSED'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_test(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST NO 535: A*B was: 516912 B was: 712 Result was: 516912 Expected result was: 726.0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_test(700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that it took quite a large number of tests before our tests could identify a software defect. Notice also that we did something clever here, when the code breaks we print data to the console and thus we now have a starting point to troubleshoot the problem.\n",
    "\n",
    "However, as I mentioned before we can only test for correctness by the bucket-load if we have some function capable of knowing what the correct output should be.  \n",
    "\n",
    "If we can't do that, then the only thing we can do with large random tests is test the code isn't throwing up any surprises.\n",
    "\n",
    "For example, we could use a line like:\n",
    "    \n",
    "    isinstance(function, {type})\n",
    "\n",
    "Code such as this couldn’t prove a function correct but it will potentially catch a bug or two. In this particular case, we have a function that is supposed to return lists, if there is some input out there that makes the function return strings instead we would want to know about it. As you may recall from the *\"importantance or error lecture\"* such testing might have been able to detect the error we had.\n",
    "\n",
    "Another thing you can do is make educated guesses as to what the correct answer should look like. For example, in the case of the divide function above we could run hundreds of tests where we check:\n",
    "    \n",
    "    divide(a, b) >= a.  \n",
    "    \n",
    "That makes sense right? if our function is dividing A by B then, even if we don't know what the answer is exactly we can nontheless put an upper bound on its size and test for that. \n",
    "\n",
    "Let's now see some of these other tests in action..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bad_list(n):\n",
    "    \"\"\"Returns a list 0 to n. \"\"\"\n",
    "    if n < 100:\n",
    "        return [7] + list(range(n))\n",
    "    elif n > 150:\n",
    "        return list(range(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, just as before, we can clearly see where and how this function will misbehave, but lets run a few tests and see if our tests can actually pin-point the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def test_list_func(n):\n",
    "    for test in range(n):\n",
    "        randomint = random.choice(range(0,200))\n",
    "        if not isinstance(bad_list(randomint), list):  \n",
    "            print(\"Test isinstance #{}: Failed, input: {}, output: {}\".format(test, randomint, bad_list(randomint)))\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            # Smart assumption test, max of list shouldn't be greater than n\n",
    "            max_test = max(bad_list(randomint)) > randomint\n",
    "            if max_test:\n",
    "                print(\"Test Max #{}: Failed. input: {}\".format(test, randomint))\n",
    "                break\n",
    "    return \"TESTING COMPLETE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this code is completing two tests at the same time. Firstly we are checking if the result is always a list. The second test is an intuitive one; if our function is supposed to return 0-to-N then clearly something has gone wrong is the max of the list is greater than n. \n",
    "\n",
    "To once again reiterate, this tests are not designed to prove correctness, but they are capable of finding a few bugs we would probably like to address. okay, lets run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test isinstance #4: Failed, input: 145, output: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TESTING COMPLETE'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list_func(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so test 4 fails, it turns out if we input 145, the function does not return a list, rather, it returns None instead. If you re-read the function this makes a lot of sense, we are asking if N < 100 or N > 150. But if N falls in the range 101-149 then both the if and elif statements are False and there isn't an else clause, so Python literally does nothing and our test function flags up the error. Rather humorously, this bug wasn't intentional! It turns out my ‘broken function’ is broken in more ways than I can imagine.\n",
    "\n",
    "Lets run the test again and see if we can catch the other error..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Max #20: Failed. input: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TESTING COMPLETE'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list_func(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So on the 20th test we get a \"max test fail\" on the input 5. And since we made the function terrible on purpose we actually know what happened. The number 7 gets added to the output and since 7 > 5 the max test flags up an error.\n",
    "\n",
    "## Doctesting\n",
    "\n",
    "The last thing to cover today is something called doctesting, this is a really quick way to making quick tests for your program.\n",
    "    \n",
    "    The Syntax:\n",
    "    \"\"\" \n",
    "    >>> {function name} ( {function argument, if any} )\n",
    "    {expected result}\n",
    "    \"\"\"\n",
    "\n",
    "And then once you have done that, you'll need to copy & paste the code below to run the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_doctests():\n",
    "    import doctest\n",
    "    doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default if all your tests pass nothing will be printed, but should a doctest fail Python will give you all the juicy detail. Lets try it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    \"\"\"\n",
    "    Returns a + b\n",
    "    \n",
    "    >>> add(10, 10)\n",
    "    20\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "run_doctests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran doctests, but since the test past nothing happened. Alright, lets show you want happens on failure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "File \"__main__\", line 3, in __main__.run_all_the_tests\n",
      "Failed example:\n",
      "    bad_list(4)\n",
      "Expected:\n",
      "    [0, 1, 2, 3]\n",
      "Got:\n",
      "    [7, 0, 1, 2, 3]\n",
      "**********************************************************************\n",
      "File \"__main__\", line 12, in __main__.run_all_the_tests\n",
      "Failed example:\n",
      "    20 + 2\n",
      "Expected:\n",
      "    23  \n",
      "Got:\n",
      "    22\n",
      "**********************************************************************\n",
      "1 items had failures:\n",
      "   2 of   4 in __main__.run_all_the_tests\n",
      "***Test Failed*** 2 failures.\n"
     ]
    }
   ],
   "source": [
    "def run_all_the_tests():   \n",
    "    \"\"\"\n",
    "    >>> bad_list(4)\n",
    "    [0, 1, 2, 3]\n",
    "    \n",
    "    >>> 1 + 1\n",
    "    2\n",
    "    \n",
    "    >>> print(True)\n",
    "    True\n",
    "    \n",
    "    >>> 20 + 2\n",
    "    23  \n",
    "    \"\"\"   \n",
    "\n",
    "    print(\"testing complete\")\n",
    "    \n",
    "run_doctests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Python ran four tests and two of them failed. It turns out 20 + 2 does not equal 23 and bad_list (surprise surprise) it up to no good. \n",
    "\n",
    "Overall, I'd recommend beginners use doctesting. Its fairly easy to use and it allows you to quickly type out basic tests for your functions.\n",
    "\n",
    "As a matter of fact, doctests are a great thing to write **BEFORE** you write the rest of your code. By spending a minute typing out a few basic test cases can help you write your code better in the first place. Why is that? Well, if you are thinking about good tests (i.e. stuff likely to break your code) at the outset, chances are you will code with that problem in mind.\n",
    "\n",
    "Alright guys, that’s everything I think I need to say about testing. If you only learn one thing from today’s lecture please let it be thing: The secret to writing good test cases is thinking hard about all the wonderful way the code could break. \n",
    "\n",
    "p.s.DONT FORGET YOUR HOMEWORK!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
